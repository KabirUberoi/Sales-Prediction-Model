{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-24T12:49:26.164999Z",
     "iopub.status.busy": "2024-11-24T12:49:26.163783Z",
     "iopub.status.idle": "2024-11-24T12:49:26.175253Z",
     "shell.execute_reply": "2024-11-24T12:49:26.173886Z",
     "shell.execute_reply.started": "2024-11-24T12:49:26.164923Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T12:49:26.776139Z",
     "iopub.status.busy": "2024-11-24T12:49:26.775595Z",
     "iopub.status.idle": "2024-11-24T12:49:26.802592Z",
     "shell.execute_reply": "2024-11-24T12:49:26.801300Z",
     "shell.execute_reply.started": "2024-11-24T12:49:26.776102Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None, var_red=None, variance=None):\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "        self.var_red = var_red  \n",
    "        self.variance = variance \n",
    "\n",
    "class DecisionTreeRegressor:\n",
    "    def __init__(self, min_samples_split=2, max_depth=2,n_features=None):\n",
    "        self.root = None\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.n_features=n_features\n",
    "\n",
    "    def build_tree(self, dataset, curr_depth=0, parent_variance=None):\n",
    "        X, Y = dataset[:, :-1], dataset[:, -1]\n",
    "        num_samples, num_features = np.shape(X)\n",
    "        if not self.n_features:\n",
    "            self.n_features = math.sqrt(num_features)\n",
    "        \n",
    "        if parent_variance is None:\n",
    "            parent_variance = np.var(Y)\n",
    "        \n",
    "        best_split = {}\n",
    "        if num_samples >= self.min_samples_split and curr_depth <= self.max_depth:\n",
    "            feature_indices = np.random.choice(num_features, self.n_features, replace=False)\n",
    "            best_split = self.get_best_split(dataset, num_samples, feature_indices, parent_variance)\n",
    "            if best_split[\"var_red\"] > 0:\n",
    "                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth + 1, best_split[\"left_variance\"])\n",
    "                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth + 1, best_split[\"right_variance\"])\n",
    "                return Node(\n",
    "                    feature_index=best_split[\"feature_index\"], \n",
    "                    threshold=best_split[\"threshold\"], \n",
    "                    left=left_subtree, \n",
    "                    right=right_subtree, \n",
    "                    var_red=best_split[\"var_red\"],\n",
    "                    variance=parent_variance\n",
    "                )\n",
    "        \n",
    "        # Create a leaf node if no valid split is found\n",
    "        leaf_value = self.calculate_leaf_value(Y)\n",
    "        return Node(value=leaf_value, variance=parent_variance)\n",
    "\n",
    "    def get_best_split(self, dataset, num_samples, feature_indices, parent_variance):\n",
    "        best_split = {\n",
    "            \"feature_index\": None,\n",
    "            \"threshold\": None,\n",
    "            \"dataset_left\": None,\n",
    "            \"dataset_right\": None,\n",
    "            \"var_red\": -float(\"inf\"),\n",
    "            \"left_variance\": None,\n",
    "            \"right_variance\": None\n",
    "        }\n",
    "\n",
    "        y = dataset[:, -1]\n",
    "        total_samples = len(y)\n",
    "        \n",
    "        for feature_index in feature_indices:\n",
    "            feature_values = dataset[:, feature_index]\n",
    "            \n",
    "            sorted_indices = np.argsort(feature_values)\n",
    "            sorted_dataset = dataset[sorted_indices]\n",
    "            sorted_y = y[sorted_indices]\n",
    "\n",
    "            prefix_sum = np.cumsum(sorted_y)\n",
    "            prefix_sq_sum = np.cumsum(sorted_y ** 2)\n",
    "\n",
    "            for i in range(1, num_samples):\n",
    "                if feature_values[sorted_indices[i]] == feature_values[sorted_indices[i - 1]]:\n",
    "                    continue  \n",
    "\n",
    "                left_count = i\n",
    "                right_count = num_samples - i\n",
    "                left_sum = prefix_sum[i - 1]\n",
    "                left_sq_sum = prefix_sq_sum[i - 1]\n",
    "                right_sum = prefix_sum[-1] - left_sum\n",
    "                right_sq_sum = prefix_sq_sum[-1] - left_sq_sum\n",
    "\n",
    "                left_var = (left_sq_sum / left_count) - (left_sum / left_count) ** 2 if left_count > 0 else 0\n",
    "                right_var = (right_sq_sum / right_count) - (right_sum / right_count) ** 2 if right_count > 0 else 0\n",
    "\n",
    "                var_red = parent_variance - (\n",
    "                    (left_count / total_samples) * left_var\n",
    "                    + (right_count / total_samples) * right_var\n",
    "                )\n",
    "\n",
    "                if var_red > best_split[\"var_red\"]:\n",
    "                    best_split = {\n",
    "                        \"feature_index\": feature_index,\n",
    "                        \"threshold\": feature_values[sorted_indices[i]],\n",
    "                        \"dataset_left\": sorted_dataset[:i],\n",
    "                        \"dataset_right\": sorted_dataset[i:],\n",
    "                        \"var_red\": var_red,\n",
    "                        \"left_variance\": left_var,\n",
    "                        \"right_variance\": right_var\n",
    "                    }\n",
    "\n",
    "        return best_split\n",
    "\n",
    "\n",
    "    def calculate_leaf_value(self, Y):\n",
    "        return np.mean(Y)\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        if len(X) == 0 or len(Y) == 0:\n",
    "            raise ValueError(\"Input data cannot be empty.\")\n",
    "        dataset = np.concatenate((X, Y.reshape(-1, 1)), axis=1)\n",
    "        self.root = self.build_tree(dataset)\n",
    "        \n",
    "    def make_prediction(self, x, tree):\n",
    "        if tree.value is not None:\n",
    "            return tree.value\n",
    "        feature_val = x[tree.feature_index]\n",
    "        if feature_val <= tree.threshold:\n",
    "            return self.make_prediction(x, tree.left)\n",
    "        else:\n",
    "            return self.make_prediction(x, tree.right)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return [self.make_prediction(x, self.root) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T12:49:27.048344Z",
     "iopub.status.busy": "2024-11-24T12:49:27.047945Z",
     "iopub.status.idle": "2024-11-24T12:49:27.060753Z",
     "shell.execute_reply": "2024-11-24T12:49:27.059286Z",
     "shell.execute_reply.started": "2024-11-24T12:49:27.048310Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class RandomForestRegressor:\n",
    "    def __init__(self, n_trees=10, max_depth=5, min_samples_split=2, n_features=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.n_features = n_features\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, Y, X_val=None, Y_val=None):\n",
    "        n_samples, n_features = X.shape\n",
    "        if self.n_features is None:\n",
    "            self.n_features = int(math.sqrt(n_features))  # Default: sqrt of total features\n",
    "        \n",
    "        for i in range(self.n_trees):\n",
    "            indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            X_sample, Y_sample = X[indices], Y[indices]\n",
    "            \n",
    "            tree = DecisionTreeRegressor(\n",
    "                min_samples_split=self.min_samples_split, \n",
    "                max_depth=self.max_depth, \n",
    "                n_features=self.n_features\n",
    "            )\n",
    "            tree.fit(X_sample, Y_sample)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "            if X_val is not None and Y_val is not None and (i + 1) % 5 == 0:\n",
    "                predictions = self.predict(X_val)\n",
    "                val_loss = np.mean((Y_val - predictions) ** 2)  # Mean squared error\n",
    "                print(f\"Iteration {i + 1}/{self.n_trees}: Validation Loss = {val_loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        return np.mean(tree_predictions, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T12:49:27.388518Z",
     "iopub.status.busy": "2024-11-24T12:49:27.386741Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g4/73wglc1561dbz4ljhlw_hlc80000gn/T/ipykernel_46671/1201578366.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_data['Item_Weight'].fillna(train_data['Item_Weight'].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7998, 1326)\n"
     ]
    }
   ],
   "source": [
    "train_path = 'SP_Train.xlsx'  \n",
    "train_data = pd.read_excel(train_path)\n",
    "\n",
    "train_data['Item_Weight'].fillna(train_data['Item_Weight'].mean(), inplace=True)\n",
    "train_data.loc[\n",
    "    (train_data['Outlet_Type'] == 'Grocery Store') & (train_data['Outlet_Size'].isna()),\n",
    "    'Outlet_Size'\n",
    "] = 'Small'\n",
    "\n",
    "train_data.loc[\n",
    "    (train_data['Outlet_Type'] == 'Supermarket Type1') & (train_data['Outlet_Size'].isna()),\n",
    "    'Outlet_Size'\n",
    "] = 'Small'\n",
    "\n",
    "train_data.loc[\n",
    "    (train_data['Outlet_Type'] == 'Supermarket Type2') & (train_data['Outlet_Size'].isna()),\n",
    "    'Outlet_Size'\n",
    "] = 'Medium'\n",
    "\n",
    "train_data.loc[\n",
    "    (train_data['Outlet_Type'] == 'Supermarket Type3') & (train_data['Outlet_Size'].isna()),\n",
    "    'Outlet_Size'\n",
    "] = 'Medium'\n",
    "\n",
    "train_data['Item_Fat_Content'] = train_data['Item_Fat_Content'].replace({'LF': 'Low Fat', 'reg': 'Regular', 'low fat': 'Low Fat'})\n",
    "\n",
    "train_data_encoded = pd.get_dummies(train_data, drop_first=True)\n",
    "\n",
    "X = train_data_encoded.drop(columns=['Item_Outlet_Sales'])\n",
    "y = np.array(train_data_encoded['Item_Outlet_Sales'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=1326, svd_solver='randomized', random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "print(X_pca.shape)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def evaluate_model(n_trees, max_depth, n_features):\n",
    "    rf = RandomForestRegressor(\n",
    "        n_trees=n_trees, max_depth=max_depth, n_features=n_features\n",
    "    )\n",
    "    rf.fit(X_train, y_train, X_val, y_val) \n",
    "    y_pred = rf.predict(X_val)\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    return mse\n",
    "\n",
    "n_trees_list = [200]\n",
    "max_depth_list = [3]\n",
    "n_features_list = [None]  # None means using root of total features\n",
    "\n",
    "best_mse = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "for n_trees in n_trees_list:\n",
    "    for max_depth in max_depth_list:\n",
    "        for n_features in n_features_list:\n",
    "            print(f\"Evaluating: n_trees={n_trees}, max_depth={max_depth}, n_features={n_features}\")\n",
    "            mse = evaluate_model(n_trees, max_depth, n_features)\n",
    "            print(f\"Mean Squared Error: {mse}\")\n",
    "            if mse < best_mse:\n",
    "                best_mse = mse\n",
    "                best_params = {'n_trees': n_trees, 'max_depth': max_depth, 'n_features': n_features}\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(f\"n_trees: {best_params['n_trees']}\")\n",
    "print(f\"max_depth: {best_params['max_depth']}\")\n",
    "print(f\"n_features: {best_params['n_features']}\")\n",
    "print(f\"Best Mean Squared Error: {best_mse}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6153516,
     "sourceId": 9997742,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
